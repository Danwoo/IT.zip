# -*- coding: utf-8 -*-
"""Data_Train_Test_Split

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tCf2z8otYSw-Jc2DevP3sZIuhpjBomE
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

book = pd.read_csv('data/book.csv', sep=',', index_col=[0])
#document = pd.read_csv('data/document.csv', sep=',', index_col=[0])
paper = pd.read_csv('data/paper.csv', sep=',', index_col=[0])



print("book shape: ", book.shape)
#print(document.shape)
print("paper shape: ", paper.shape)

book.columns = ['passage', 'summary']
#document.columns = ['passage', 'summary']
paper.columns = ['passage', 'summary']

book['label'] = 0
#document['label'] = 1
paper['label'] = 2

a1 = []
a2 = []
for i in range(book.shape[0]):
    a1.append(len(book['passage'].iloc[i]))
for j in range(paper.shape[0]):
    a2.append(len(paper['passage'].iloc[j]))

'''
print('book_length')
print(a1.index(np.max(np.array(a1))))
print(book['passage'].iloc[a1.index(max(a1))])
print()
print(book['passage'].iloc[a1.index(min(a1))])
a1 = np.array(a1)
print('max: ', np.max(a1))
print('min: ', np.min(a1))
print('avg: ', np.average(a1))
print('sig: ', np.std(a1))

u = 0
for i in range(np.size(a1)):
    if a1[i] > 500 and a1[i] < 1200:
        u += 1
print(u/np.size(a1))

print('paper_length')
print(a2.index(np.max(np.array(a2))))
print(paper['passage'].iloc[a2.index(max(a2))])
print()
print(paper['passage'].iloc[a2.index(min(a2))])
a2 = np.array(a2)
print('max: ', np.max(a2))
print('min: ', np.min(a2))
print('avg: ', np.average(a2))
print('sig: ', np.std(a2))

v = 0
for i in range(np.size(a2)):
    if a2[i] > 500 and a2[i] < 1200:
    # if a2[i] > 3000:
        v += 1
print(v/np.size(a2))
'''

data = pd.concat([book, paper]).reset_index(drop=True)
#data = pd.concat([book, document, paper])
#data = data.head(300)
print("data shape: ",data.shape)
print(data.head(10))

keywords = pd.read_csv('data/사전.csv', usecols=[0], encoding='ms949', header=None)
keyword_data1 = pd.read_csv('data/keywords.txt', header=None)
keyword_data1 = pd.DataFrame(keyword_data1, columns=[0])
keywords = pd.concat([keywords, keyword_data1]).reset_index(drop=True)
print("keywords size:", keywords.shape)
print(keywords.head(10))

# print(sum(keywords[keywords==0]))
ind = []
n = 0
for i in range(data.shape[0]):
  if any(s in str(data['passage'].iloc[i]) for s in keywords.iloc[:,0]):
    ind.append(1)
  else:
    ind.append(0)
  # print("i is ", i)
  # data.drop([i], axis=0)
  if i % 10000 == 0:
      n += 1
      print(f"check_point : {n} / {round(data.shape[0], -5) / 10000}")

print("총 선별 데이터 개수 : ", len(ind))
data = data.head(len(ind))
print(data.shape)
data['keyword'] = ind
data = data[data['keyword'] == 1]


print(data.shape)

train_set, validation_set = train_test_split(data, test_size=0.33, random_state=43, shuffle=True, stratify=data['label'])
validation_set, test_set = train_test_split(validation_set, test_size=0.5, random_state=43, shuffle=True, stratify=validation_set['label'])

print(train_set.shape)
print(validation_set.shape)
print(test_set.shape)

print(np.sum(train_set['label']==0))
print(np.sum(train_set['label']==1))
print(np.sum(train_set['label']==2))

print(np.sum(validation_set['label']==0))
print(np.sum(validation_set['label']==1))
print(np.sum(validation_set['label']==2))

print(np.sum(test_set['label']==0))
print(np.sum(test_set['label']==1))
print(np.sum(test_set['label']==2))

# data_sample = train_set.head(100)
# print(np.sum(data_sample['label']==0))
# print(np.sum(data_sample['label']==1))
# print(np.sum(data_sample['label']==2))
# data_sample.to_csv(path_or_buf='data/sample_100.csv', sep=',', encoding='utf-8-sig', columns=['passage','summary'])

train_set.to_csv(path_or_buf='data/data_train_l.csv', sep=',', encoding='utf-8-sig', columns=['passage','summary','label'])
validation_set.to_csv(path_or_buf='data/data_valid_l.csv', sep=',', encoding='utf-8-sig', columns=['passage','summary','label'])
test_set.to_csv(path_or_buf='data/data_test_l.csv', sep=',', encoding='utf-8-sig', columns=['passage','summary','label'])

